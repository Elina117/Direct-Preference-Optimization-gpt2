# Описание проекта
  В данном проекте реализован метод DPO (Direct Preference Optimization) для дообучения языковой модели GPT-2 с целью улучшения качества ее ответов на основе пар предпочтений. DPO напрямую оптимизирует политику модели так, чтоб она отдавала предпочтение "chosen" данным, без явного определения функции вознаграждения и сложных RL алгоритмов. 
Для дообучения используется датасет "Anthropic HH-RLHF" (7000 - train, 3000 - validation), соедржащий пары под ключами chosen и rejected, объедененные с promt.

# Подробное описание написанных блоков кода:
1. Класс DPOModel
   - загружаем с помозью hugging face языковую модель gpt-2, загружаем копию этой модели для последующего использования как референсной;
   - токенизируем пары promt + chosen/rejected, затем прогоняем их через модель и сдвигаем полученные метки и логиты на один токен для предсказания следующего. Вычиясляем логарифмы вероятностей для правильных токенов и суммируем по всей последовательности для каждого входа;
   - в функции обучения для каждого батча берутся пары текстов, для референсной и обучаемой модели вычисляем логарифмическое правдоводобие, по формуле DPO считаем разницу и бинарную кросс-энтропию для улучшения качества.
   - в режиме оценки, без обновления градиентов валидируем модель;
   - обновлем параметры модели.
  
  2. Класс DPODataModule
    - этот класс служит для обработки датасета и подготовки его для обучения и валидации.

  3. Обучение
    - обучение проходит на GPU, на 5 эпохах, валидация происходит после каждой эпохи.

  4. Анализ результата
     - визуализирую изменение accuracy и 
     - проверяю внешне ответы на один promt от референсной и обученной модели.
    
  5. Результат
      Лосс на тренировке постепенно уменьшалась с примерно 5 до 2.1, что значит, модель училась делать меньше ошибок на тренировочных данных. Accuracy на тренировке тоже росла — с около 53% до 61%, показывая, что модель все лучше отличает хорошие ответы от плохих. На валидации лосс сначала был низкий, но потом сильно колебался, а точность оставалась низкой — около 0–33%, что может указывать на сложности модели с обобщением на новых данных.
      На визуальной оценке ответов на вопросы модель до обучения отвечала конкретно и уверенно, выражала свое мнение, а после обучения ответ стал менее прямолинейным и склонной к рассуждениям.


# Инструкция по воспроизведению результатов

Этот проект реализует обучение языковой модели с помощью метода Direct Preference Optimization (DPO) на датасете `anthropic/hh-rlhf`. Код написан в виде Jupyter ноутбука и тестировался на GPU Nvidia Tesla P100 в среде Kaggle.

## Требования

- Python 3.8+  
- GPU
- Установленные библиотеки из `requirements.txt` 

## Установка зависимостей

1. Склонируйте репозиторий:

   ```bash
   git clone <URL_репозитория>
   cd <название_репозитория>

2. Установите зависимости
   ```bash
   git clone <URL_репозитория>
   cd <название_репозитория>

3. Откройте jupyter notebook (например, в Kaggle, Collab или локально с GPU)
    ```bash
   jupyter notebook

## Используемое оборудование
- **GPU:** Nvidia Tesla P100 (Kaggle)

## Время обучения
- Примерно 30-40 минут на 5 эпох

## Входные данные
- Используется публичный датасет `anthropic/hh-rlhf`  
- Загрузка происходит автоматически через библиотеку `datasets`

     
